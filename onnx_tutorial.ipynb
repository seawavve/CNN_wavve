{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_load_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMar+YmglxkzBeqf6R5mzVD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seawavve/CNN_wavve/blob/main/onnx_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train MNIST"
      ],
      "metadata": {
        "id": "QJ8C0dfv-yun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Test loss: 0.023477373644709587\n",
        "Test accuracy: 0.9954000115394592\n",
        "'''\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import sys\n",
        "from keras.utils import np_utils\n",
        "import keras\n",
        "import tensorflow.keras as tk\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "import numpy as np \n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "print('Python version : ', sys.version)\n",
        "print('Keras version : ', keras.__version__)\n",
        "\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "(x_train, y_train), (x_test, y_test) = tk.datasets.mnist.load_data()\n",
        "\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "filename='checkpoint'.format(epochs,batch_size)\n",
        "early_stopping=EarlyStopping(monitor='val_loss',mode='min',patience=15,verbose=1)                           #얼리스타핑\n",
        "checkpoint=ModelCheckpoint(filename,monitor='val_loss',verbose=1,save_best_only=True,mode='auto')           #체크포인트\n",
        "\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "#1\n",
        "model.add(Conv2D(64, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "#2\n",
        "model.add(Conv2D(64, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "#3\n",
        "model.add(Conv2D(64, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#4\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test) , callbacks=[checkpoint,early_stopping])\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:',  score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# model.save('MNIST_CNN_model')"
      ],
      "metadata": {
        "id": "UYjpUrsl9nyh",
        "outputId": "9f315851-f8a9-4bb6-a22f-e7315f7edb3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version :  3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "Keras version :  2.7.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 64)        640       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 28, 28, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 28, 28, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              3137000   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 1000)             4000      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                10010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,226,274\n",
            "Trainable params: 3,223,890\n",
            "Non-trainable params: 2,384\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1727 - accuracy: 0.9488\n",
            "Epoch 00001: val_loss improved from inf to 0.46652, saving model to checkpoint\n",
            "INFO:tensorflow:Assets written to: checkpoint/assets\n",
            "469/469 [==============================] - 22s 25ms/step - loss: 0.1727 - accuracy: 0.9488 - val_loss: 0.4665 - val_accuracy: 0.8557\n",
            "Epoch 2/10\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9803\n",
            "Epoch 00002: val_loss improved from 0.46652 to 0.04373, saving model to checkpoint\n",
            "INFO:tensorflow:Assets written to: checkpoint/assets\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0645 - accuracy: 0.9803 - val_loss: 0.0437 - val_accuracy: 0.9861\n",
            "Epoch 3/10\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.0456 - accuracy: 0.9852\n",
            "Epoch 00003: val_loss improved from 0.04373 to 0.02624, saving model to checkpoint\n",
            "INFO:tensorflow:Assets written to: checkpoint/assets\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0454 - accuracy: 0.9852 - val_loss: 0.0262 - val_accuracy: 0.9916\n",
            "Epoch 4/10\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.0400 - accuracy: 0.9871\n",
            "Epoch 00004: val_loss improved from 0.02624 to 0.02526, saving model to checkpoint\n",
            "INFO:tensorflow:Assets written to: checkpoint/assets\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0399 - accuracy: 0.9872 - val_loss: 0.0253 - val_accuracy: 0.9920\n",
            "Epoch 5/10\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9891\n",
            "Epoch 00005: val_loss improved from 0.02526 to 0.02119, saving model to checkpoint\n",
            "INFO:tensorflow:Assets written to: checkpoint/assets\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0337 - accuracy: 0.9891 - val_loss: 0.0212 - val_accuracy: 0.9929\n",
            "Epoch 6/10\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.0314 - accuracy: 0.9901\n",
            "Epoch 00006: val_loss did not improve from 0.02119\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.0314 - accuracy: 0.9901 - val_loss: 0.0439 - val_accuracy: 0.9878\n",
            "Epoch 7/10\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.0301 - accuracy: 0.9903\n",
            "Epoch 00007: val_loss did not improve from 0.02119\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.0300 - accuracy: 0.9903 - val_loss: 0.0269 - val_accuracy: 0.9909\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9912\n",
            "Epoch 00008: val_loss did not improve from 0.02119\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.0267 - accuracy: 0.9912 - val_loss: 0.0276 - val_accuracy: 0.9921\n",
            "Epoch 9/10\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9914\n",
            "Epoch 00009: val_loss did not improve from 0.02119\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.0270 - accuracy: 0.9913 - val_loss: 0.0241 - val_accuracy: 0.9917\n",
            "Epoch 10/10\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9923\n",
            "Epoch 00010: val_loss improved from 0.02119 to 0.01974, saving model to checkpoint\n",
            "INFO:tensorflow:Assets written to: checkpoint/assets\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.0197 - val_accuracy: 0.9937\n",
            "Test loss: 0.019740432500839233\n",
            "Test accuracy: 0.9937000274658203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "JYEKb7N4-2ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('./checkpoint')\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model_shapes.png', show_shapes=True)\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy=\", score[1])"
      ],
      "metadata": {
        "id": "0gqv4HjK-pxE",
        "outputId": "8fde29d8-3f73-4280-a689-0b03db518f14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy= 0.9937000274658203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer TF to ONNX"
      ],
      "metadata": {
        "id": "OL0OVq_--6G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U tf2onnx\n",
        "! python -m tf2onnx.convert --saved-model checkpoint --output model.onnx"
      ],
      "metadata": {
        "id": "fryqj1T4--8W",
        "outputId": "56ab48fd-fb76-4664-a13e-d3269a55a535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.9.3-py3-none-any.whl (435 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 419 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 430 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 435 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting flatbuffers~=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.7/dist-packages (from tf2onnx) (1.19.5)\n",
            "Collecting onnx>=1.4.1\n",
            "  Downloading onnx-1.10.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx>=1.4.1->tf2onnx) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.4.1->tf2onnx) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tf2onnx) (3.0.4)\n",
            "Installing collected packages: onnx, flatbuffers, tf2onnx\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "Successfully installed flatbuffers-1.12 onnx-1.10.2 tf2onnx-1.9.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flatbuffers"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2022-01-18 11:59:15,983 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2022-01-18 11:59:16,658 - INFO - Signatures found in model: [serving_default].\n",
            "2022-01-18 11:59:16,658 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2022-01-18 11:59:16,658 - INFO - Output names: ['dense_1']\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2022-01-18 11:59:16,937 - WARNING - From /usr/local/lib/python3.7/dist-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "2022-01-18 11:59:17,104 - INFO - Using tensorflow=2.7.0, onnx=1.10.2, tf2onnx=1.9.3/1190aa\n",
            "2022-01-18 11:59:17,104 - INFO - Using opset <onnx, 9>\n",
            "2022-01-18 11:59:17,847 - INFO - Computed 0 values for constant folding\n",
            "2022-01-18 11:59:18,314 - INFO - Optimizing ONNX model\n",
            "2022-01-18 11:59:18,424 - INFO - After optimization: Cast -1 (1->0), Const +1 (25->26), Identity -9 (9->0), Reshape +1 (1->2), Transpose -15 (16->1)\n",
            "2022-01-18 11:59:18,437 - INFO - \n",
            "2022-01-18 11:59:18,437 - INFO - Successfully converted TensorFlow model checkpoint to ONNX\n",
            "2022-01-18 11:59:18,437 - INFO - Model inputs: ['conv2d_input']\n",
            "2022-01-18 11:59:18,437 - INFO - Model outputs: ['dense_1']\n",
            "2022-01-18 11:59:18,437 - INFO - ONNX model is saved at model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX to TensorRT"
      ],
      "metadata": {
        "id": "S1B8OrxkElvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python --version"
      ],
      "metadata": {
        "id": "bVVGztNAF8rP",
        "outputId": "9ad7228c-3bd8-4f2c-adb3-70a4eaa6b7b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 -m pip install onnx==1.8.0"
      ],
      "metadata": {
        "id": "P4ibbyqrGZOM",
        "outputId": "8549d2a1-a268-425d-ef40-ba23eb311168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx==1.8.0\n",
            "  Downloading onnx-1.8.0-cp37-cp37m-manylinux2010_x86_64.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx==1.8.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from onnx==1.8.0) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx==1.8.0) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from onnx==1.8.0) (1.15.0)\n",
            "Installing collected packages: onnx\n",
            "  Attempting uninstall: onnx\n",
            "    Found existing installation: onnx 1.10.2\n",
            "    Uninstalling onnx-1.10.2:\n",
            "      Successfully uninstalled onnx-1.10.2\n",
            "Successfully installed onnx-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import onnx\n",
        "# import onnx_tensorrt.backend as backend\n",
        "# import numpy as np\n",
        "\n",
        "# model = onnx.load(\"model.onnx\")\n",
        "# engine = backend.prepare(model, device='CUDA:0')\n",
        "# input_data = np.random.random(size=(32, 3, 224, 224)).astype(np.float32)\n",
        "# output_data = engine.run(input_data)[0]\n",
        "# print(output_data)\n",
        "# print(output_data.shape)"
      ],
      "metadata": {
        "id": "h2u2vuypEob0"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}